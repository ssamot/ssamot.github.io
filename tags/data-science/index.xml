<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data science on ssamot's heretical rumblings</title><link>https://ssamot.me/tags/data-science/</link><description>Recent content in Data science on ssamot's heretical rumblings</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><copyright>Spyros Samothrakis</copyright><lastBuildDate>Thu, 23 Jun 2022 10:00:07 +0100</lastBuildDate><atom:link href="https://ssamot.me/tags/data-science/index.xml" rel="self" type="application/rss+xml"/><item><title>Cookie cutter data science</title><link>https://ssamot.me/posts/ccds/</link><pubDate>Thu, 23 Jun 2022 10:00:07 +0100</pubDate><guid>https://ssamot.me/posts/ccds/</guid><description>I &amp;rsquo;ve recently discovered https://github.com/drivendata/cookiecutter-data-science/ &amp;ndash; an effort to standardise the pipeline of a data science project. The practices proposed seem ideal to me &amp;ndash; from a very sane directory structure to discussing makefiles as project pipeline DAGs.
Not sure how I managed to survive using completely custom project templates for each project. In fact, I will argue, one should go ahead and automate the whole report/presentation generation pipeline. There aren&amp;rsquo;t that many data science patterns anyway (let&amp;rsquo;s say regression/classification/causal inference/time-series analysis/arbitrary optimisation), so it shouldn&amp;rsquo;t be that hard.</description></item></channel></rss>