<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Artificial Intelligence on ssamot's heretical rumblings</title><link>https://ssamot.me/tags/artificial-intelligence/</link><description>Recent content in Artificial Intelligence on ssamot's heretical rumblings</description><generator>Hugo</generator><language>en</language><copyright>Spyros Samothrakis</copyright><lastBuildDate>Wed, 01 Nov 2023 21:49:58 +0000</lastBuildDate><atom:link href="https://ssamot.me/tags/artificial-intelligence/index.xml" rel="self" type="application/rss+xml"/><item><title>Regulating AI Research</title><link>https://ssamot.me/posts/regulate/</link><pubDate>Wed, 01 Nov 2023 21:49:58 +0000</pubDate><guid>https://ssamot.me/posts/regulate/</guid><description>&lt;p>In one of the most bizarre turns, it seems certain vested interests, propped by the ultra-religious, want to regulate AI research. Quoting Julian: &lt;a href="https://togelius.blogspot.com/2023/11/ai-safety-regulation-threatens-our.html">https://togelius.blogspot.com/2023/11/ai-safety-regulation-threatens-our.html&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>I don&amp;rsquo;t know how you feel about this. I think this prospect is absolutely horrible. I think it is worth sacrificing almost anything to avoid this future, which would make Stasi and Big Brother blush. Luckily, in my estimate we don&amp;rsquo;t need to sacrifice anything, because there is no credible existential threat from AI. It is all figments of the hyperactive imaginations of some people, boosted by certain corporations who develop AI models and stand to win from regulating away their competition.&lt;/p></description></item><item><title>Apocalypse and AI</title><link>https://ssamot.me/posts/apo/</link><pubDate>Wed, 26 Apr 2023 12:45:10 +0100</pubDate><guid>https://ssamot.me/posts/apo/</guid><description>&lt;p>To the extent that I can draw an analogy, there is a pretty clear pattern taking place in prophecy, and it goes something like this:&lt;/p>
&lt;ol>
&lt;li>Mark / Matthew / Paul: Second coming in one generation, new earthly kingdom of justice ASAP&lt;/li>
&lt;li>Luke: The kingdom of God is within you&lt;/li>
&lt;li>John: It&amp;rsquo;s all gone wrong, nothing is clear, we only have the word to rely on, the devil (or his father?) rules this world&lt;/li>
&lt;li>Apocalypse: It will happen and you will all suffer and die&lt;/li>
&lt;li>Origen: Most meaningful progress is after our death anyway&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;li>Church fathers: It has already happened, the Kingdom of God has arrived etc (i.e. full preterism).&lt;/li>
&lt;/ol>
&lt;p>It seems we are at stage 1 of the cycle, but at some point the inevitable realisation that very little is going to happen will sink in.&lt;/p></description></item><item><title>Reinforcement learning</title><link>https://ssamot.me/posts/rl/</link><pubDate>Sat, 15 Apr 2023 12:09:59 +0100</pubDate><guid>https://ssamot.me/posts/rl/</guid><description>&lt;p>Deep down I am a control/RL junkie, but I&amp;rsquo;ll give it to the NLP crowd, they have managed to turn AI mainstream and create a useful tool. It&amp;rsquo;s not that we are not seeing really cool RL projects (see here: &lt;a href="https://clemenswinter.com/2023/04/14/entity-based-reinforcement-learning/">https://clemenswinter.com/2023/04/14/entity-based-reinforcement-learning/&lt;/a>, here: &lt;a href="https://github.com/NVIDIA-Omniverse/IsaacGymEnvs">https://github.com/NVIDIA-Omniverse/IsaacGymEnvs&lt;/a> and here: &lt;a href="https://arxiv.org/abs/2304.01315)">https://arxiv.org/abs/2304.01315)&lt;/a>, it&amp;rsquo;s that they do not have much use outside the lab (yet).&lt;/p>
&lt;p>In a sense, we still do not have the killer RL application, and as I have said in another post, we will not get one unless zero-shot RL is solved.&lt;/p></description></item><item><title>The manifold hypothesis and text</title><link>https://ssamot.me/posts/manifolods/</link><pubDate>Fri, 17 Feb 2023 07:50:32 +0000</pubDate><guid>https://ssamot.me/posts/manifolods/</guid><description>&lt;p>ChatGPT (and most GPT-like services) make up stuff all the time &amp;ndash; it&amp;rsquo;s not that surprising, and should not be that hard to fix. What is surprising is that language can be abstracted in some smooth manifold, and tons of data reveal the correct one. In a sense, quantity has a quality of its own, and larger models do really well because they merely (!?) need to interpolate between training data examples.&lt;/p></description></item><item><title>Beliefs vs AI</title><link>https://ssamot.me/posts/believers/</link><pubDate>Thu, 02 Feb 2023 14:13:54 +0000</pubDate><guid>https://ssamot.me/posts/believers/</guid><description>&lt;p>The oft-quoted comment from &lt;a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow">Thinking, Fast and Slow&lt;/a>, referring to the financial services community, reads like:&lt;/p>
&lt;blockquote>
&lt;p>We know that people can maintain an unshakable faith in any proposition, however absurd, when they are sustained by a community of like-minded believers&lt;/p>
&lt;/blockquote>
&lt;p>It&amp;rsquo;s a great quote, but it&amp;rsquo;s not clear to me how the data in the book supports it. It also has very little to do with the argument the book is bringing forward, which is that we have two modes of thinking (i.e. dual-process theory), a fast &amp;ldquo;quick intuitive&amp;rdquo; vs a slow &amp;ldquo;deep thinking&amp;rdquo;. Dual process theory is nicely abstracted in what I would now consider a classic reinforcement learning article termed
&lt;a href="https://proceedings.neurips.cc/paper/2017/hash/d8e1344e27a5b08cdfd5d027d9b8d6de-Abstract.html">Thinking Fast and Slow with Deep Learning and Tree Search&lt;/a>, which introduces expert iteration, something that later become known as alphaGO, and creates hex-playing agents.&lt;/p></description></item><item><title>We can't have nice things</title><link>https://ssamot.me/posts/cauchy/</link><pubDate>Fri, 14 Oct 2022 20:22:42 +0100</pubDate><guid>https://ssamot.me/posts/cauchy/</guid><description>&lt;p>One of the clearest tweet threads on why self-driving cars are hard is the one below. I do no have anything to add, but that someone should do the simulations and with real simulated features and write a paper about it. It would make for a very entertaining read&amp;hellip;&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">For years I&amp;#39;ve been claiming that our &lt;a href="https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw">#AI&lt;/a> being based in statistics is not equipped for dealing real world data. That data is most likely fat-tailed, and hence not possible to characterize by sampling. Let me explain this in thread below: 👇&lt;/p></description></item><item><title>Grounding large language models</title><link>https://ssamot.me/posts/grounding/</link><pubDate>Wed, 29 Jun 2022 12:51:51 +0100</pubDate><guid>https://ssamot.me/posts/grounding/</guid><description>&lt;p>I&amp;rsquo;ve came across this paper: &lt;a href="https://openreview.net/pdf?id=gJcEM8sxHK">Mapping Language Models to Grounded Conceptual Spaces&lt;/a> while searching for methods to address the problem of grounding large language models. Seems like the general approach is to feed the model data post-training. It&amp;rsquo;s interesting, as it implies that you can learn from purely mental constructs link them to reality later on.&lt;/p></description></item><item><title>Natural language interfaces for the command line and privacy</title><link>https://ssamot.me/posts/llmcmd/</link><pubDate>Wed, 22 Jun 2022 06:15:54 +0100</pubDate><guid>https://ssamot.me/posts/llmcmd/</guid><description>&lt;p>Microsoft released a natural language interface for the command line (&lt;a href="https://github.com/microsoft/Codex-CLI)">https://github.com/microsoft/Codex-CLI)&lt;/a>, while a past effort can been seen here: &lt;a href="https://github.com/tom-doerr/zsh_codex">https://github.com/tom-doerr/zsh_codex&lt;/a>. What I find worrying is that there is currently no way of using a local model &amp;mdash; these services do remote calls. Which means that you are bound to call OpenAI&amp;rsquo;s APIs, which pretty much guarantees loss of privacy. It looks like alternatives are doing much on the privacy front &amp;ndash; see here: &lt;a href="https://www.tabnine.com/">https://www.tabnine.com/&lt;/a>.&lt;/p></description></item><item><title>Large language models - GPT-3 this time</title><link>https://ssamot.me/posts/nlp2/</link><pubDate>Fri, 17 Jun 2022 07:26:17 +0100</pubDate><guid>https://ssamot.me/posts/nlp2/</guid><description>&lt;p>So I&amp;rsquo;ve started playing with GPT-3 this morning, posing the same questions I&amp;rsquo;ve asked GPT-J. Seems like the answers are much more coherent.
For default configuration parameters I get reasonable stuff, but the model has no concept if something is real or not and makes things up:&lt;/p>
&lt;p>&amp;ndash; Q: How do I make steel?&lt;/p>
&lt;ul>
&lt;li>A: There are many ways to make steel. The most common method is to start with pig iron. Pig iron is created in a blast furnace by melting iron ore with coke (a carbon-rich substance made from coal) and limestone (a calcium-rich rock).&lt;/li>
&lt;/ul>
&lt;p>&amp;ndash; Q: How much carbon is needed to make steel?&lt;/p></description></item><item><title>Large language models</title><link>https://ssamot.me/posts/nlp/</link><pubDate>Tue, 14 Jun 2022 22:22:42 +0100</pubDate><guid>https://ssamot.me/posts/nlp/</guid><description>&lt;p>I really don&amp;rsquo;t get all that hype around large language models. I can understand their uses in NLP (and their superb transfer learning powers), but people are treating them like the end-goal of AI. I might be biased, but they don&amp;rsquo;t look that smart to me &amp;ndash; I&amp;rsquo;ve tried this one here: &lt;a href="https://6b.eleuther.ai/">https://6b.eleuther.ai/&lt;/a>. You get different results based on what parameters you use, but:&lt;/p>
&lt;p>Model with top-p = 1.0, temperature = 0 (i.e. choose best &amp;ndash; I guess that&amp;rsquo;s what it means &amp;ndash; you get really bizarre results with the default website parameters)&lt;/p></description></item><item><title>Labour theory of value through the lens of reinforcement learning</title><link>https://ssamot.me/posts/ltv/</link><pubDate>Mon, 13 Jun 2022 07:09:59 +0100</pubDate><guid>https://ssamot.me/posts/ltv/</guid><description>&lt;p>One of the popular online debates has been on the usefulness of the labour theory of value (LTV). I&amp;rsquo;ll attempt to give LTV a reinforcement learning (RL) spin (that&amp;rsquo;s how I understand it anyway) which should hopefully clarify things a bit. I&amp;rsquo;ll also abuse notation.&lt;/p>
&lt;p>Imagine a scenario where an agent (an abstraction of a capitalist, firm, upper management etc) has access to the following at every time step $t$:&lt;/p></description></item><item><title>Goodhart’s Law and AI</title><link>https://ssamot.me/posts/goodheart/</link><pubDate>Fri, 10 Jun 2022 14:18:52 +0100</pubDate><guid>https://ssamot.me/posts/goodheart/</guid><description>&lt;p>OpenAI has a blog post &lt;a href="https://openai.com/blog/measuring-goodharts-law">here&lt;/a> &amp;ndash; quoting from the article:&lt;/p>
&lt;blockquote>
&lt;p>Goodhart’s law famously says: “When a measure becomes a target, it ceases to be a good measure.” Although originally from economics, it’s something we have to grapple with at OpenAI when figuring out how to optimize objectives that are difficult or costly to measure. It’s often necessary to introduce some proxy objective that’s easier or cheaper to measure, but when we do this, we need to be careful not to optimize it too much.&lt;/p></description></item><item><title>Coding the matrix</title><link>https://ssamot.me/posts/cm/</link><pubDate>Mon, 06 Jun 2022 19:40:11 +0100</pubDate><guid>https://ssamot.me/posts/cm/</guid><description>&lt;p>I thoroughly admire efforts to turn mathematics into code &amp;ndash; in the legacy of &lt;a href="http://codingthematrix.com/">Coding the Matrix&lt;/a>. A new book is being released on the topic of Logic: &lt;a href="https://www.cambridge.org/gb/academic/subjects/computer-science/programming-languages-and-applied-logic/mathematical-logic-through-python">Mathematical Logic through Python&lt;/a>. Proof assistants (the likes of &lt;a href="https://github.com/leanprover/lean">Lean&lt;/a>) are being continuously developed and becoming more and more mainstream by the hour. I am pretty sure developments in these fronts will revolutionise the teaching of mathematics, making them accessible to whole groups of hobbyists, the same way that autodiff revolutionised ML.&lt;/p></description></item><item><title>Neats vs scruffies vs money</title><link>https://ssamot.me/posts/neats/</link><pubDate>Sat, 28 May 2022 06:05:29 +0100</pubDate><guid>https://ssamot.me/posts/neats/</guid><description>&lt;p>I&amp;rsquo;ve stumbled upon &amp;ldquo;&lt;a href="https://www.research.ed.ac.uk/en/publications/what-is-the-well-dressed-ai-educator-wearing-now">What is the well dressed educator wearing now?&lt;/a>&amp;rdquo;, an AI education article written in 1982, through a twitter thread. The article can be summed up as a commentary on whether one should pursue theoretical methods vs a more experimental approach (i.e. neats vs scruffies) and has been done to death since the article was written, so I don&amp;rsquo;t have much new to say. What astonished me is the actual focus on education. The piece reads like something written in another universe. There is no link to outcomes (which invariably means how many students will we get if we follow approach A vs approach B or where will these students get jobs). It&amp;rsquo;s like the authors only concern is what scientifically valid AI education looks like.&lt;/p></description></item><item><title>Big data</title><link>https://ssamot.me/posts/bigdata/</link><pubDate>Fri, 27 May 2022 09:31:24 +0100</pubDate><guid>https://ssamot.me/posts/bigdata/</guid><description>&lt;p>Most interesting problems I have encountered in my real-life expeditions involve small data. Big data seems to me like a problem devised by certain tech companies and picked up as a trend by people that don&amp;rsquo;t need it, forcing them to use tools of extreme complexity, and as a result delaying project deployments.&lt;/p></description></item><item><title>Automation keeps being predicted</title><link>https://ssamot.me/posts/autom/</link><pubDate>Thu, 19 May 2022 06:42:54 +0100</pubDate><guid>https://ssamot.me/posts/autom/</guid><description>&lt;p>&lt;figure class="floatleft">&lt;img src="https://pbs.twimg.com/media/FTB6EoeXsAEfMy8?format=jpg&amp;amp;name=large"
 alt="Automation fears in the 50s, found on twitter, see https://twitter.com/abenanav/status/1526854505136021505">&lt;figcaption>
 &lt;p>Automation fears in the 50s, found on twitter, see &lt;a href="https://twitter.com/abenanav/status/1526854505136021505">https://twitter.com/abenanav/status/1526854505136021505&lt;/a>&lt;/p>
 &lt;/figcaption>
&lt;/figure>
 McKinsey (the consultancy) has a job automation chart &lt;a href="https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/where-machines-could-replace-humans-and-where-they-cant-yet">here&lt;/a>, posted in 2016. There seems to be a lingering fear of automation, as this old New York Time article, amply named &lt;a href="https://www.nytimes.com/1955/04/08/archives/automation-puts-industry-on-eve-of-fantastic-robot-era-its-effect.html">&amp;ldquo;Automation Puts Industry on Eve of Fantastic Robot Era; Its Effect on Workers Spurs Unions&amp;rsquo; Drive for Annual Wage&amp;rdquo;&lt;/a> seems to imply. Notice how McKinsey speculates that managers are secure from automation, though I doubt this is for the right reasons. Middle management is automated to death (think: algorithms and uber), but upper management is the one driving any automation forward and somehow I greatly doubt they will automate their own jobs. Rodney Brooks (of &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0921889005800259">Elephants don&amp;rsquo;t play chess&lt;/a> fame) keeps an automation &lt;a href="https://rodneybrooks.com/predictions-scorecard-2022-january-01/">score card&lt;/a> &amp;ndash; makes for a very interesting read for a more grounded perspective.&lt;/p></description></item><item><title>Failed prophecies</title><link>https://ssamot.me/posts/world/</link><pubDate>Tue, 10 May 2022 14:14:02 +0100</pubDate><guid>https://ssamot.me/posts/world/</guid><description>&lt;h2 id="sanctify-them-in-the-truth-john-1717">&amp;ldquo;Sanctify them in the truth&amp;rdquo; (John 17:17)&lt;/h2>
&lt;p>Tesla (the car business) recently published a video of its &lt;a href="https://www.youtube.com/watch?v=j0z4FweCy4M">AI day&lt;/a> giving a tour de force of modern artificial intelligence (AI) methods. From monte carlo tree search to simulation-based reasoning, Tesla is throwing every modern technique to the problem of self driving cars, coupled with possibly some of the best research scientists and engineers in the business. Yet, somehow, this is not enough; as another &lt;a href="https://www.youtube.com/watch?v=mfharlIQ2qE">video mocking Tesla&lt;/a> for its inability to produce a self-driving car points out, the promise of real-world AI delivering anything that would resemble its science fiction counterpart is astoundingly far. The likes of self-driving cars, &lt;a href="https://www.zdnet.com/article/elon-musks-open-source-openai-were-working-on-a-robot-that-will-do-your-household-chores/">robotic maids and butlers&lt;/a>, robotic farm work, &lt;a href="https://qz.com/2016153/ai-promised-to-revolutionize-radiology-but-so-far-its-failing/">automated medical doctors&lt;/a>, robotic you-name-it are currently a pipe dream, and might remain so for the ages (more on this later). Tesla operates within the mantle of extreme technological optimism, an almost default positions given the level of change the world has seen since the industrial revolution. This level of optimism is not unique to Tesla &amp;ndash; the techno-social landscape has been a source of unparalleled optimism across the political spectrum. Living in a world without progress is beyond our ability even contemplate, but this might be exactly what we might be facing.&lt;/p></description></item></channel></rss>