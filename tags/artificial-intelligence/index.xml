<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Artificial Intelligence on ssamot's heretical rumblings</title><link>https://ssamot.me/tags/artificial-intelligence/</link><description>Recent content in Artificial Intelligence on ssamot's heretical rumblings</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><copyright>Spyros Samothrakis</copyright><lastBuildDate>Fri, 17 Feb 2023 07:50:32 +0000</lastBuildDate><atom:link href="https://ssamot.me/tags/artificial-intelligence/index.xml" rel="self" type="application/rss+xml"/><item><title>The manifold hypothesis and text</title><link>https://ssamot.me/posts/manifolods/</link><pubDate>Fri, 17 Feb 2023 07:50:32 +0000</pubDate><guid>https://ssamot.me/posts/manifolods/</guid><description>ChatGPT (and most GPT-like services) make up stuff all the time &amp;ndash; it&amp;rsquo;s not that surprising, and should not be that hard to fix. What is surprising is that language can be abstracted within them. In a sense, quantity has a quality of its own, and larger models do really well because they merely (!?) need to interpolate between training data examples.
I wonder if RL will have the same fate &amp;ndash; there are some hints here and there: https://openreview.</description></item><item><title>Beliefs vs AI</title><link>https://ssamot.me/posts/believers/</link><pubDate>Thu, 02 Feb 2023 14:13:54 +0000</pubDate><guid>https://ssamot.me/posts/believers/</guid><description>The oft-quoted comment from Thinking, Fast and Slow, referring to the financial services community, reads like:
We know that people can maintain an unshakable faith in any proposition, however absurd, when they are sustained by a community of like-minded believers
It&amp;rsquo;s a great quote, but it&amp;rsquo;s not clear to me how the data in the book supports it. It also has very little to do with the argument the book is bringing forward, which is that we have two modes of thinking (i.</description></item><item><title>We can't have nice things</title><link>https://ssamot.me/posts/cauchy/</link><pubDate>Fri, 14 Oct 2022 20:22:42 +0100</pubDate><guid>https://ssamot.me/posts/cauchy/</guid><description>One of the clearest tweet threads on why self-driving cars are hard is the one below. I do no have anything to add, but that someone should do the simulations and with real simulated features and write a paper about it. It would make for a very entertaining read&amp;hellip;
For years I&amp;#39;ve been claiming that our #AI being based in statistics is not equipped for dealing real world data. That data is most likely fat-tailed, and hence not possible to characterize by sampling.</description></item><item><title>Grounding large language models</title><link>https://ssamot.me/posts/grounding/</link><pubDate>Wed, 29 Jun 2022 12:51:51 +0100</pubDate><guid>https://ssamot.me/posts/grounding/</guid><description>I&amp;rsquo;ve came across this paper: Mapping Language Models to Grounded Conceptual Spaces while searching for methods to address the problem of grounding large language models. Seems like the general approach is to feed the model data post-training. It&amp;rsquo;s interesting, as it implies that you can learn from purely mental constructs link them to reality later on.</description></item><item><title>Natural language interfaces for the command line and privacy</title><link>https://ssamot.me/posts/llmcmd/</link><pubDate>Wed, 22 Jun 2022 06:15:54 +0100</pubDate><guid>https://ssamot.me/posts/llmcmd/</guid><description>Microsoft released a natural language interface for the command line (https://github.com/microsoft/Codex-CLI), while a past effort can been seen here: https://github.com/tom-doerr/zsh_codex. What I find worrying is that there is currently no way of using a local model &amp;mdash; these services do remote calls. Which means that you are bound to call OpenAI&amp;rsquo;s APIs, which pretty much guarantees loss of privacy. It looks like alternatives are doing much on the privacy front &amp;ndash; see here: https://www.</description></item><item><title>Large language models - GPT-3 this time</title><link>https://ssamot.me/posts/nlp2/</link><pubDate>Fri, 17 Jun 2022 07:26:17 +0100</pubDate><guid>https://ssamot.me/posts/nlp2/</guid><description>So I&amp;rsquo;ve started playing with GPT-3 this morning, posing the same questions I&amp;rsquo;ve asked GPT-J. Seems like the answers are much more coherent. For default configuration parameters I get reasonable stuff, but the model has no concept if something is real or not and makes things up:
&amp;ndash; Q: How do I make steel?
A: There are many ways to make steel. The most common method is to start with pig iron.</description></item><item><title>Large language models</title><link>https://ssamot.me/posts/nlp/</link><pubDate>Tue, 14 Jun 2022 22:22:42 +0100</pubDate><guid>https://ssamot.me/posts/nlp/</guid><description>I really don&amp;rsquo;t get all that hype around large language models. I can understand their uses in NLP (and their superb transfer learning powers), but people are treating them like the end-goal of AI. I might be biased, but they don&amp;rsquo;t look that smart to me &amp;ndash; I&amp;rsquo;ve tried this one here: https://6b.eleuther.ai/. You get different results based on what parameters you use, but:
Model with top-p = 1.0, temperature = 0 (i.</description></item><item><title>Labour theory of value through the lens of reinforcement learning</title><link>https://ssamot.me/posts/ltv/</link><pubDate>Mon, 13 Jun 2022 07:09:59 +0100</pubDate><guid>https://ssamot.me/posts/ltv/</guid><description>One of the popular online debates has been on the usefulness of the labour theory of value (LTV). I&amp;rsquo;ll attempt to give LTV a reinforcement learning (RL) spin (that&amp;rsquo;s how I understand it anyway) which should hopefully clarify things a bit. I&amp;rsquo;ll also abuse notation.
Imagine a scenario where an agent (an abstraction of a capitalist, firm, upper management etc) has access to the following at every time step $t$:</description></item><item><title>Goodhart’s Law and AI</title><link>https://ssamot.me/posts/goodheart/</link><pubDate>Fri, 10 Jun 2022 14:18:52 +0100</pubDate><guid>https://ssamot.me/posts/goodheart/</guid><description>OpenAI has a blog post here &amp;ndash; quoting from the article:
Goodhart’s law famously says: “When a measure becomes a target, it ceases to be a good measure.” Although originally from economics, it’s something we have to grapple with at OpenAI when figuring out how to optimize objectives that are difficult or costly to measure. It’s often necessary to introduce some proxy objective that’s easier or cheaper to measure, but when we do this, we need to be careful not to optimize it too much.</description></item><item><title>Coding the matrix</title><link>https://ssamot.me/posts/cm/</link><pubDate>Mon, 06 Jun 2022 19:40:11 +0100</pubDate><guid>https://ssamot.me/posts/cm/</guid><description>I thoroughly admire efforts to turn mathematics into code &amp;ndash; in the legacy of Coding the Matrix. A new book is being released on the topic of Logic: Mathematical Logic through Python. Proof assistants (the likes of Lean) are being continuously developed and becoming more and more mainstream by the hour. I am pretty sure developments in these fronts will revolutionise the teaching of mathematics, making them accessible to whole groups of hobbyists, the same way that autodiff revolutionised ML.</description></item><item><title>Neats vs scruffies vs money</title><link>https://ssamot.me/posts/neats/</link><pubDate>Sat, 28 May 2022 06:05:29 +0100</pubDate><guid>https://ssamot.me/posts/neats/</guid><description>I&amp;rsquo;ve stumbled upon &amp;ldquo;What is the well dressed educator wearing now?&amp;rdquo;, an AI education article written in 1982, through a twitter thread. The article can be summed up as a commentary on whether one should pursue theoretical methods vs a more experimental approach (i.e. neats vs scruffies) and has been done to death since the article was written, so I don&amp;rsquo;t have much new to say. What astonished me is the actual focus on education.</description></item><item><title>Big data</title><link>https://ssamot.me/posts/bigdata/</link><pubDate>Fri, 27 May 2022 09:31:24 +0100</pubDate><guid>https://ssamot.me/posts/bigdata/</guid><description>Most interesting problems I have encountered in my real-life expeditions involve small data. Big data seems to me like a problem devised by certain tech companies and picked up as a trend by people that don&amp;rsquo;t need it, forcing them to use tools of extreme complexity, and as a result delaying project deployments.</description></item><item><title>Automation keeps being predicted</title><link>https://ssamot.me/posts/autom/</link><pubDate>Thu, 19 May 2022 06:42:54 +0100</pubDate><guid>https://ssamot.me/posts/autom/</guid><description>Automation fears in the 50s, found on twitter, see https://twitter.com/abenanav/status/1526854505136021505
McKinsey (the consultancy) has a job automation chart here, posted in 2016. There seems to be a lingering fear of automation, as this old New York Time article, amply named &amp;ldquo;Automation Puts Industry on Eve of Fantastic Robot Era; Its Effect on Workers Spurs Unions&amp;rsquo; Drive for Annual Wage&amp;rdquo; seems to imply. Notice how McKinsey speculates that managers are secure from automation, though I doubt this is for the right reasons.</description></item><item><title>Failed prophecies</title><link>https://ssamot.me/posts/world/</link><pubDate>Tue, 10 May 2022 14:14:02 +0100</pubDate><guid>https://ssamot.me/posts/world/</guid><description>&amp;ldquo;Sanctify them in the truth&amp;rdquo; (John 17:17) Tesla (the car business) recently published a video of its AI day giving a tour de force of modern artificial intelligence (AI) methods. From monte carlo tree search to simulation-based reasoning, Tesla is throwing every modern technique to the problem of self driving cars, coupled with possibly some of the best research scientists and engineers in the business. Yet, somehow, this is not enough; as another video mocking Tesla for its inability to produce a self-driving car points out, the promise of real-world AI delivering anything that would resemble its science fiction counterpart is astoundingly far.</description></item></channel></rss>