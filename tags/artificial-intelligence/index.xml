<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Artificial Intelligence on ssamot's heretical rumblings</title><link>https://ssamot.me/tags/artificial-intelligence/</link><description>Recent content in Artificial Intelligence on ssamot's heretical rumblings</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><copyright>Spyros Samothrakis</copyright><lastBuildDate>Wed, 22 Jun 2022 06:15:54 +0100</lastBuildDate><atom:link href="https://ssamot.me/tags/artificial-intelligence/index.xml" rel="self" type="application/rss+xml"/><item><title>Natural language interfaces for the command line and privacy</title><link>https://ssamot.me/posts/llmcmd/</link><pubDate>Wed, 22 Jun 2022 06:15:54 +0100</pubDate><guid>https://ssamot.me/posts/llmcmd/</guid><description>Microsoft released a natural language interface for the command line (https://github.com/microsoft/Codex-CLI), while a past effort can been seen here: https://github.com/tom-doerr/zsh_codex. What I find worrying is that there is currently no way of using a local model &amp;mdash; these services do remote calls. Which means that you are bound to call OpenAI&amp;rsquo;s APIs (or, maybe, other competitors in the future &amp;ndash; see here https://www.tabnine.com/), which pretty much guarantees loss of privacy.</description></item><item><title>Large language models - GPT-3 this time</title><link>https://ssamot.me/posts/nlp2/</link><pubDate>Fri, 17 Jun 2022 07:26:17 +0100</pubDate><guid>https://ssamot.me/posts/nlp2/</guid><description>So I&amp;rsquo;ve started playing with GPT-3 this morning, posing the same questions I&amp;rsquo;ve asked GPT-J. Seems like the answers are much more coherent. For default configuration parameters I get reasonable stuff, but the model has no concept if something is real or not and makes things up:
&amp;ndash; Q: How do I make steel?
A: There are many ways to make steel. The most common method is to start with pig iron.</description></item><item><title>Large language models</title><link>https://ssamot.me/posts/nlp/</link><pubDate>Tue, 14 Jun 2022 22:22:42 +0100</pubDate><guid>https://ssamot.me/posts/nlp/</guid><description>I really don&amp;rsquo;t get all that hype around large language models. I can understand their uses in NLP (and their superb transfer learning powers), but people are treating them like the end-goal of AI. I might be biased, but they don&amp;rsquo;t look that smart to me &amp;ndash; I&amp;rsquo;ve tried this one here: https://6b.eleuther.ai/. You get different results based on what parameters you use, but:
Model with top-p = 1.0, temperature = 0 (i.</description></item><item><title>Labour theory of value through the lens of reinforcement learning</title><link>https://ssamot.me/posts/ltv/</link><pubDate>Mon, 13 Jun 2022 07:09:59 +0100</pubDate><guid>https://ssamot.me/posts/ltv/</guid><description>One of the popular online debates has been on the usefulness of the labour theory of value (LTV). I&amp;rsquo;ll attempt to give LTV a reinforcement learning (RL) spin (that&amp;rsquo;s how I understand it anyway) which should hopefully clarify things a bit. I&amp;rsquo;ll also abuse notation.
Imagine a scenario where an agent (an abstraction of a capitalist, firm, upper management etc) has access to the following at every time step $t$:</description></item><item><title>Goodhart’s Law and AI</title><link>https://ssamot.me/posts/goodheart/</link><pubDate>Fri, 10 Jun 2022 14:18:52 +0100</pubDate><guid>https://ssamot.me/posts/goodheart/</guid><description>OpenAI has a blog post here &amp;ndash; quoting from the article:
Goodhart’s law famously says: “When a measure becomes a target, it ceases to be a good measure.” Although originally from economics, it’s something we have to grapple with at OpenAI when figuring out how to optimize objectives that are difficult or costly to measure. It’s often necessary to introduce some proxy objective that’s easier or cheaper to measure, but when we do this, we need to be careful not to optimize it too much.</description></item><item><title>Coding the matrix</title><link>https://ssamot.me/posts/cm/</link><pubDate>Mon, 06 Jun 2022 19:40:11 +0100</pubDate><guid>https://ssamot.me/posts/cm/</guid><description>I thoroughly admire efforts to turn mathematics into code &amp;ndash; in the legacy of Coding the Matrix. A new book is being released on the topic of Logic: Mathematical Logic through Python. Proof assistants (the likes of Lean) are being continuously developed and becoming more and more mainstream by the hour. I am pretty sure developments in these fronts will revolutionise the teaching of mathematics, making them accessible to whole groups of hobbyists, the same way that autodiff revolutionised ML.</description></item><item><title>Neats vs scruffies vs money</title><link>https://ssamot.me/posts/neats/</link><pubDate>Sat, 28 May 2022 06:05:29 +0100</pubDate><guid>https://ssamot.me/posts/neats/</guid><description>I&amp;rsquo;ve stumbled upon &amp;ldquo;What is the well dressed educator wearing now?&amp;rdquo;, an AI education article written in 1982, through a twitter thread. The article can be summed up as a commentary on whether one should pursue theoretical methods vs a more experimental approach (i.e. neats vs scruffies) and has been done to death since the article was written, so I don&amp;rsquo;t have much new to say. What astonished me is the actual focus on education.</description></item><item><title>Big data</title><link>https://ssamot.me/posts/bigdata/</link><pubDate>Fri, 27 May 2022 09:31:24 +0100</pubDate><guid>https://ssamot.me/posts/bigdata/</guid><description>Most interesting problems I have encountered in my real-life expeditions involve small data. Big data seems to me like a problem devised by certain tech companies and picked up as a trend by people that don&amp;rsquo;t need it, forcing them to use tools of extreme complexity, and as a result delaying project deployment.</description></item><item><title>Automation keeps being predicted</title><link>https://ssamot.me/posts/autom/</link><pubDate>Thu, 19 May 2022 06:42:54 +0100</pubDate><guid>https://ssamot.me/posts/autom/</guid><description>Automation fears in the 50s, found on twitter, see https://twitter.com/abenanav/status/1526854505136021505
McKinsey (the consultancy) has a job automation chart here, posted in 2016. There seems to be a lingering fear of automation, as this old New York Time article, amply named &amp;ldquo;Automation Puts Industry on Eve of Fantastic Robot Era; Its Effect on Workers Spurs Unions&amp;rsquo; Drive for Annual Wage&amp;rdquo; seems to imply. Notice how McKinsey speculates that managers are secure from automation, though I doubt this is for the right reasons.</description></item><item><title>Failed prophecies</title><link>https://ssamot.me/posts/world/</link><pubDate>Tue, 10 May 2022 14:14:02 +0100</pubDate><guid>https://ssamot.me/posts/world/</guid><description>&amp;ldquo;Sanctify them in the truth&amp;rdquo; (John 17:17) Tesla (the car business) recently published a video of its AI day giving a tour de force of modern artificial intelligence (AI) methods. From monte carlo tree search to simulation-based reasoning, Tesla is throwing every modern technique to the problem of self driving cars, coupled with possibly some of the best research scientists and engineers in the business. Yet, somehow, this is not enough; as another video mocking Tesla for its inability to produce a self-driving car points out, the promise of real-world AI delivering anything that would resemble its science fiction counterpart is astoundingly far.</description></item></channel></rss>