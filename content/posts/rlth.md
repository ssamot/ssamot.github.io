+++
title = "Thoughts on RL and AI for 2023"
date = "2023-01-04T10:11:32Z"
author = "ssamot"
tags = ["AI"]
description = "..."
+++

Rodney Brooks has updated his scorecard for 2023: https://rodneybrooks.com/predictions-scorecard-2023-january-01/. He is more or less spot on (I think his blog is where I first came across the idea that self-driving cars are a strict lower bound to AGI). The demise of Argo AI https://www.usnews.com/news/business/articles/2022-10-26/ford-cuts-investment-in-autonomous-vehicle-unit-posts-loss, alongside a host of really ambitious companies in the last few years, might act as a signal that the "market" is to abandon some of the more adventurous commercial efforts around AI. 

So what would be my take on why AI prophecies did not materialise? I think we still do not know how to do proper feature transformations from sensory inputs to a reasonable latent space, that can then be used to help with ultra-fast generalisation. I am not sure if this would come from meta-learning or some neuroscientist telling us how the brain does it, but without it the flat tails of real-world will keep ruining most of our efforts. 