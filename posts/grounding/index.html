<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Grounding large language models - ssamot's heretical rumblings</title><link rel=icon type=image/png href=/final_logo_purple_inv_sat.png><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="...might be achievable"><meta property="og:image" content><meta property="og:title" content="Grounding large language models"><meta property="og:description" content="...might be achievable"><meta property="og:type" content="article"><meta property="og:url" content="https://ssamot.me/posts/grounding/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-29T12:51:51+01:00"><meta property="article:modified_time" content="2022-06-29T12:51:51+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Grounding large language models"><meta name=twitter:description content="...might be achievable"><script src=https://ssamot.mejs/feather.min.js></script><link href=https://ssamot.me/css/fonts.352cefb34094cd6aeabccf0709d7c4006d075638fcd84db93818af5a51eec660.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://ssamot.me/css/main.74589d0801ac2e0077442cb10044eed50980086bdb7742d01991b816a420536d.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://ssamot.me/css/dark.512da9ec5e8a9c5da6bffed7d277ab45998fc7e791baebc0567864afdf75993d.css><script data-goatcounter=https://ssamot.goatcounter.com/count async src=//gc.zgo.at/count.js></script><noscript><img src="https://ssamot.goatcounter.com/count?p=/posts/grounding"></noscript></head><body><div class=content><header><div class=main><a href=https://ssamot.me>ssamot's heretical rumblings</a></div><nav><a href=/about>about</a>
<a href=/contact>contact</a>
<a href=/posts>posts</a>
<a href=/publications>publications</a></nav></header><main><article><div class=title><h1 class=title>Grounding large language models</h1><div class=meta>Posted on Jun 29, 2022</div></div><section class=body><p>I&rsquo;ve came across this paper: <a href="https://openreview.net/pdf?id=gJcEM8sxHK">Mapping Language Models to Grounded Conceptual Spaces</a> while searching for methods to address the problem of grounding large language models. Seems like the general approach is to feed the model data post-training. It&rsquo;s interesting, as it implies that you can learn from purely mental constructs link them to reality later on.</p></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/artificial-intelligence>Artificial Intelligence</a></li></ul></nav></div></article></main><footer><div class=footer-info><a href=https://twitter.com/spysamot/>Twitter</a>
<a href=https://github.com/ssamot/>GitHub</a>&nbsp|| based on 

    <a href=https://gohugo.io>Hugo</a>/<a href=https://github.com/athul/archie>Archie</a> |
2024 Spyros Samothrakis |</div></footer><script>feather.replace()</script></div></body></html>