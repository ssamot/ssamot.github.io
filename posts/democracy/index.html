<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Democracy and reinforcement learning - ssamot's heretical rumblings</title><link rel=icon type=image/png href=/final_logo_purple_inv_sat.png><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="...are intertwined"><meta property="og:image" content><meta property="og:url" content="https://ssamot.me/posts/democracy/"><meta property="og:site_name" content="ssamot's heretical rumblings"><meta property="og:title" content="Democracy and reinforcement learning"><meta property="og:description" content="...are intertwined"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-14T16:25:13+01:00"><meta property="article:modified_time" content="2022-08-14T16:25:13+01:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="Democracy"><meta name=twitter:card content="summary"><meta name=twitter:title content="Democracy and reinforcement learning"><meta name=twitter:description content="...are intertwined"><script src=https://ssamot.me/js/feather.min.js></script><link href=https://ssamot.me/css/fonts.352cefb34094cd6aeabccf0709d7c4006d075638fcd84db93818af5a51eec660.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://ssamot.me/css/main.74589d0801ac2e0077442cb10044eed50980086bdb7742d01991b816a420536d.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://ssamot.me/css/dark.512da9ec5e8a9c5da6bffed7d277ab45998fc7e791baebc0567864afdf75993d.css><script data-goatcounter=https://ssamot.goatcounter.com/count async src=//gc.zgo.at/count.js></script><noscript><img src="https://ssamot.goatcounter.com/count?p=/posts/democracy"></noscript></head><body><div class=content><header><div class=main><a href=https://ssamot.me/>ssamot's heretical rumblings</a></div><nav><a href=/about>about</a>
<a href=/contact>contact</a>
<a href=/posts>posts</a>
<a href=/publications>publications</a></nav></header><main><article><div class=title><h1 class=title>Democracy and reinforcement learning</h1><div class=meta>Posted on Aug 14, 2022</div></div><section class=body><p>In the age-old battle between representative vs direct democracy, two systems stand out. In representative democracy, it seems <a href=https://rangevoting.org/WarrenSmithPages/homepage/rangevote.pdf>range voting</a>, where each candidate is given a score from 0 to 99 by each voter, has certain desirable properties. In (direct) democracy, the way Aristotle describes it in <a href=http://classics.mit.edu/Aristotle/politics.html>Politics</a>, no elections take place but a group of qualified candidates (which can presumably be the whole population) is subsampled randomly. The argument for random subsampling is that people tend to vote for wealthy and famous individuals, so getting someone to represent the &ldquo;common man&rdquo; requires just picking someone in random and giving them an admin post.</p><p>Why institute these mechanisms in the first place? The idea is that as a society, we should have a way to express our preferences, the &ldquo;genuine&rdquo; will of the people. Both systems look flawed to me, as they allow for malicious players to dominate without much difficulty. In the representative democracy case this can be achieved by using excessive propaganda, in the direct democracy case by taking over the institutions that make up the state; individuals of strong ability and will have an incentive to move to institutional (but unelected) positions and gain power there. This in turn would require some kind of ostracism (a form of negative voting) to get rid of them, which which might prove undesirable.</p><p>There is somewhere a full research programme ready to be launched on how collective decisions can be made that do not overly favour certain individuals. Potentially a good starting point could be <a href=ftp://ftp.cs.brown.edu/pub/techreports/05/cs05-08.pdf>Greenwald, Amy, Keith Hall, and Martin Zinkevich. &ldquo;Correlated Q-Learning.&rdquo; (2005)</a>. They define four types of equilibria a system can stabilise in:</p><blockquote><ol><li>utilitarian: maximize the sum of all agents’ rewards</li><li>egalitarian: maximize the minimum of all agents’ rewards</li><li>plutocratic: maximize the maximum of all agents’ rewards</li><li>dictatorial: maximize the maximum of any individual agent’s rewards</li></ol></blockquote><p>A good voting system should allow for a mix of egalitarian and utilitarian equilibria, and thus push society forward collectively. You could potentially set this up as a full simulation where different voting schemes come into play, with propaganda also playing a prominent role, and try to find out how coalition formations would play out as the system moves from one set of equlibria to another.</p></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/ai>AI</a></li><li><a href=/tags/democracy>Democracy</a></li></ul></nav></div></article></main><footer><div class=footer-info><a href=https://twitter.com/spysamot/>Twitter</a>
<a href=https://github.com/ssamot/>GitHub</a>&nbsp|| based on 

    <a href=https://gohugo.io>Hugo</a>/<a href=https://github.com/athul/archie>Archie</a> |
2024 Spyros Samothrakis |</div></footer><script>feather.replace()</script></div></body></html>